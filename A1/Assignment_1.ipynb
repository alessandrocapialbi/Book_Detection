{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandrocapialbi/Book_Detection/blob/main/A1/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Group members\n",
        "\n",
        "|  Name   |  Surname   |     Email                            |    Student ID                                             |\n",
        "| :-----: | :--------: | :----------------------------------: | :-----------------------------------------------------: |\n",
        "| Ludovico  | Gorrieri   | `ludovico.gorrieri@studio.unibo.it`   |  To Be Determinned |\n",
        "| Alessandro  | Capialbi | `alessandro.capialbi@studio.unibo.it`  | 0001191564 |\n",
        "| Faezeh  | Sarlakifar | `faezeh.sarlakifar@studio.unibo.it`  | 0001164608 |"
      ],
      "metadata": {
        "id": "lL0v8o62Zmn2"
      },
      "id": "lL0v8o62Zmn2"
    },
    {
      "cell_type": "markdown",
      "id": "fef9ca24",
      "metadata": {
        "id": "fef9ca24"
      },
      "source": [
        "## Task 1 & 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a4faa3e",
      "metadata": {
        "id": "2a4faa3e"
      },
      "source": [
        "### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://github.com/nlp-unibo/nlp-course-material/tree/main/2025-2026/Assignment%201/data"
      ],
      "metadata": {
        "id": "-SyxdgzJZVoA"
      },
      "id": "-SyxdgzJZVoA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nlp-unibo/nlp-course-material.git\n",
        "%cd \"nlp-course-material/2025-2026/Assignment 1\""
      ],
      "metadata": {
        "id": "FOzj0GigrzVh",
        "outputId": "a69c4aa1-6741-4ce1-f014-a57e3233834f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FOzj0GigrzVh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nlp-course-material'...\n",
            "remote: Enumerating objects: 391, done.\u001b[K\n",
            "remote: Counting objects: 100% (391/391), done.\u001b[K\n",
            "remote: Compressing objects: 100% (288/288), done.\u001b[K\n",
            "remote: Total 391 (delta 174), reused 294 (delta 90), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (391/391), 8.56 MiB | 20.16 MiB/s, done.\n",
            "Resolving deltas: 100% (174/174), done.\n",
            "/content/nlp-course-material/2025-2026/Assignment 1/data/nlp-course-material/2025-2026/Assignment 1/nlp-course-material/2025-2026/Assignment 1/nlp-course-material/2025-2026/Assignment 1/nlp-course-material/2025-2026/Assignment 1/nlp-course-material/2025-2026/Assignment 1/nlp-course-material/2025-2026/Assignment 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tweet Preprocessing and Label Aggregation Script**\n",
        "\n",
        "This script prepares the dataset of tweets for NLP tasks.\n",
        "It handles text cleaning, tokenization, lemmatization, and label aggregation for supervised learning.\n",
        "Below is a detailed explanation of each section.\n",
        "\n",
        "# 1. Importing Required Libraries:\n",
        "\n",
        "    a) pandas / numpy → for data manipulation.\n",
        "    b) re → regular expressions for text cleaning.\n",
        "    c) nltk → for tokenization, POS tagging, and lemmatization.\n",
        "    d) Counter → to count occurrences of labels and select the majority vote.\n",
        "\n",
        "# 2. Preparing NLTK Resources:\n",
        "\n",
        "    This block ensures that all required NLTK corpora and models are available locally.\n",
        "    If a resource is missing, it is automatically downloaded.\n",
        "\n",
        "# 3. Initializing Tools:\n",
        "\n",
        "    WhitespaceTokenizer → splits text based on spaces (useful after cleaning).\n",
        "    WordNetLemmatizer → reduces words to their base or dictionary form using WordNet.\n",
        "\n",
        "# 4. Helper Function: get_wordnet_key(pos_tag):\n",
        "    This function maps Penn Treebank POS tags (e.g., NN, VB, JJ) to WordNet’s format (noun, verb, adjective, adverb).\n",
        "    This step is essential because WordNetLemmatizer requires the part of speech to perform accurate lemmatization.\n",
        "\n",
        "# 5. Lemmatization Function: lem_text(row):\n",
        "    This function:\n",
        "\n",
        "    1) Tokenizes the tweet into words.\n",
        "    2) Assigns POS tags using NLTK’s pos_tag.\n",
        "    3) Lemmatizes each word according to its part of speech.\n",
        "    4) Returns the lemmatized tweet as a single string.\n",
        "\n",
        "# 6. Cleaning Function: cleaner(row):\n",
        "\n",
        "    Purpose: Remove noise and standardize text before analysis.\n",
        "\n",
        "    Steps:\n",
        "\n",
        "    1)\tlower(): Converts all text to lowercase\n",
        "    2)\tRemove URLs: Regex https?:\\/\\/.\\S+ removes URLs and links\n",
        "    3)\tRemove mentions & hashtags:\tRegex [@#].\\S+ removes @user and #topic\n",
        "    4)\tRemove emojis/symbols: Unicode ranges cover emoticons, flags, pictographs\n",
        "    5)\tRemove non-alphanumeric:\tKeeps only letters, digits, and spaces\n",
        "    6)\tNormalize whitespace:\tCollapses multiple spaces into one\n",
        "\n",
        "# 7. Label Aggregation:\n",
        "\n",
        "    This part aggregates multiple label votes for a tweet into a single numeric label.\n",
        "\n",
        "    How it works:\n",
        "\n",
        "    1) For each row, it collects all values in labels_task2 except \"UNKNOWN\".\n",
        "    2) Uses Counter to find the most common label (majority vote).\n",
        "    3) Maps that label to a numerical ID using the mapping dictionary.\n",
        "\n",
        "# Summary\n",
        "\n",
        "    This script prepares tweets by performing:\n",
        "\n",
        "    1) cleaner():\tRemove unwanted characters and normalize text\n",
        "    2) lem_text():\tLemmatize words for consistent representation\n",
        "    3) aggregator():\tConvert multiple annotations into a single label\n",
        "\n",
        "    Together, these functions create a clean, normalized, and labeled dataset,\n",
        "    ideal for tasks like text classication that we will perform.\n"
      ],
      "metadata": {
        "id": "ZmzGl4wOn-uG"
      },
      "id": "ZmzGl4wOn-uG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ed34f3e",
      "metadata": {
        "id": "0ed34f3e"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import (word_tokenize,\n",
        "                            sent_tokenize,\n",
        "                            WhitespaceTokenizer);\n",
        "\n",
        "# Prepare NLTK\n",
        "resources = [\n",
        "    ('corpora/omw-1.4', 'omw-1.4'),\n",
        "    ('corpora/wordnet', 'wordnet'),\n",
        "    ('taggers/averaged_perceptron_tagger', 'averaged_perceptron_tagger'),\n",
        "    ('taggers/averaged_perceptron_tagger_eng', 'averaged_perceptron_tagger_eng'),\n",
        "    ('tokenizers/punkt_tab', 'punkt_tab'),\n",
        "    ('tokenizers/punkt', 'punkt')\n",
        "]\n",
        "\n",
        "for resource_path, download_name in resources:\n",
        "    try:\n",
        "        nltk.data.find(resource_path)\n",
        "    except LookupError:\n",
        "        nltk.download(download_name, quiet=True)\n",
        "\n",
        "tokenizer = WhitespaceTokenizer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def get_wordnet_key(pos_tag):\n",
        "    if pos_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif pos_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif pos_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif pos_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return 'n'\n",
        "\n",
        "# Lemmatize a row's tweet\n",
        "def lem_text(row):\n",
        "    tokens = tokenizer.tokenize(row.tweet)\n",
        "    tagged = pos_tag(tokens)\n",
        "    words = [lemmatizer.lemmatize(word, get_wordnet_key(tag))\n",
        "             for word, tag in tagged]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Clean a row's tweet\n",
        "def cleaner(row):\n",
        "    text = row.tweet\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'https?:\\/\\/.\\S+', '', text)\n",
        "    text = re.sub(r'[@#].\\S+', '', text)\n",
        "    text = re.sub(\n",
        "        \"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # Flags\n",
        "                                    \"]+\", '', text\n",
        "    )\n",
        "    text = re.sub(r'[^a-z^0-9^\\s]*', '', text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Aggregate the labels (labels_task2)\n",
        "aggregator = lambda row: \\\n",
        "    mapping[Counter([vote for vote in row.labels_task2 if vote != \"UNKNWON\"]).most_common(1)[0][0]]\n",
        "\n",
        "mapping = {\n",
        "    '-': 0,\n",
        "    'DIRECT': 1,\n",
        "    'JUDGEMENTAL': 2,\n",
        "    'REPORTED': 3\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8058981d",
      "metadata": {
        "id": "8058981d"
      },
      "source": [
        "### Clean, split and lemmatize the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f431cee3",
      "metadata": {
        "id": "f431cee3"
      },
      "outputs": [],
      "source": [
        "# Load the files\n",
        "with open(\"data/training.json\", \"r\") as tr, \\\n",
        "     open(\"data/validation.json\", \"r\") as te, \\\n",
        "     open(\"data/test.json\", \"r\") as va:\n",
        "    train_json = json.load(tr)\n",
        "    val_json = json.load(te)\n",
        "    test_json = json.load(va)\n",
        "\n",
        "# Create the dataframes (setting the index to id_EXIST)\n",
        "dts = {\n",
        "    \"train\": pd.DataFrame.from_dict(train_json, orient=\"index\").set_index(\"id_EXIST\"),\n",
        "    \"test\": pd.DataFrame.from_dict(test_json, orient=\"index\").set_index(\"id_EXIST\"),\n",
        "    \"val\": pd.DataFrame.from_dict(val_json, orient=\"index\").set_index(\"id_EXIST\")\n",
        "}\n",
        "\n",
        "# Unnecessary columns\n",
        "drop_cols = [\"number_annotators\", \"annotators\", \"gender_annotators\",\n",
        "    \"age_annotators\", \"labels_task1\", \"labels_task3\", \"split\"]\n",
        "\n",
        "# Clean and lemmatize the data\n",
        "for name, df in dts.items():\n",
        "    df = df[df.lang == \"en\"] # Drop spanish.\n",
        "\n",
        "    df = df.drop(columns=drop_cols) # Drop unnecessary cols.\n",
        "\n",
        "    df[\"labels\"] = df.apply(aggregator, axis=1) # Aggregate the labels (maj. voting).\n",
        "    df = df.drop(columns=\"labels_task2\")\n",
        "\n",
        "    for func in [cleaner, lem_text]:\n",
        "        df[\"tweet\"] = df.apply(func, axis=1) # Clean the tweets.\n",
        "\n",
        "    dts[name] = df\n",
        "\n",
        "train, test, val = dts.values()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Text Encoding"
      ],
      "metadata": {
        "id": "vEnqOYsCYlvU"
      },
      "id": "vEnqOYsCYlvU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "K7k51at6l6_D"
      },
      "id": "K7k51at6l6_D"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRUv6BySgXVn",
        "outputId": "f4f4e663-f666-4624-b6e5-99073d188b9c"
      },
      "id": "IRUv6BySgXVn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "!unzip -q glove.twitter.27B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33eb2c15-70f8-4eaa-f295-2a8585959f73",
        "id": "iEWaLM81l33Y"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-24 05:02:46--  http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.twitter.27B.zip [following]\n",
            "--2025-10-24 05:02:46--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\n",
            "--2025-10-24 05:02:46--  https://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520408563 (1.4G) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.zip’\n",
            "\n",
            "glove.twitter.27B.z 100%[===================>]   1.42G  4.93MB/s    in 4m 48s  \n",
            "\n",
            "2025-10-24 05:07:35 (5.03 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563]\n",
            "\n"
          ]
        }
      ],
      "id": "iEWaLM81l33Y"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "tf_data = tf.data\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3O0p0dvSeEZ9"
      },
      "id": "3O0p0dvSeEZ9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
      ],
      "metadata": {
        "id": "0FplWisheHyu"
      },
      "id": "0FplWisheHyu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the vocabulary"
      ],
      "metadata": {
        "id": "RxS1xQEAbOzF"
      },
      "id": "RxS1xQEAbOzF"
    },
    {
      "cell_type": "code",
      "source": [
        "texts = train[\"tweet\"].values\n",
        "labels = train[\"labels\"].values\n",
        "\n",
        "text_ds = tf.data.Dataset.from_tensor_slices((texts, labels)).batch(64)"
      ],
      "metadata": {
        "id": "EUztF_7JeeW4"
      },
      "id": "EUztF_7JeeW4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = layers.TextVectorization(max_tokens=20000, output_sequence_length=100)\n",
        "vectorizer.adapt(text_ds.map(lambda x, y: x))"
      ],
      "metadata": {
        "id": "Vuc0yDfMYex3"
      },
      "id": "Vuc0yDfMYex3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = vectorizer.get_vocabulary()\n",
        "print(vocab[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_EcMYhmfTxV",
        "outputId": "1825757d-3721-4c7c-e854-ccc292cd3985"
      },
      "id": "I_EcMYhmfTxV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', np.str_('be'), np.str_('the'), np.str_('a'), np.str_('to'), np.str_('and'), np.str_('of'), np.str_('i'), np.str_('it')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use GloVe Embedding vectors"
      ],
      "metadata": {
        "id": "A16P9r0vggvr"
      },
      "id": "A16P9r0vggvr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert GloVe format to Word2Vec format"
      ],
      "metadata": {
        "id": "uRzg97PjhMNF"
      },
      "id": "uRzg97PjhMNF"
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding dimension: 100 (For now I want to test this one, then I'll change this hyperparameter to get better results)\n",
        "glove_file = \"glove.twitter.27B.100d.txt\"\n",
        "\n",
        "# Load GloVe into Gensim\n",
        "twitter_glove = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)\n",
        "\n",
        "print(f\"Loaded Twitter GloVe with {len(twitter_glove.key_to_index):,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcDxmFJmiJk0",
        "outputId": "0131cbcd-fed1-4c29-d84a-213beea2128b"
      },
      "id": "rcDxmFJmiJk0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Twitter GloVe with 1,193,514 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build TensorFlow embedding matrix"
      ],
      "metadata": {
        "id": "_zMU_mosjYVz"
      },
      "id": "_zMU_mosjYVz"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = vectorizer.get_vocabulary()\n",
        "embedding_dim = twitter_glove.vector_size\n",
        "embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
        "\n",
        "for i, word in enumerate(vocab):\n",
        "    if word in twitter_glove:\n",
        "        embedding_matrix[i] = twitter_glove[word]"
      ],
      "metadata": {
        "id": "d-YhtOpbjbTt"
      },
      "id": "d-YhtOpbjbTt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OOV handling"
      ],
      "metadata": {
        "id": "wX5jUvUsiC5z"
      },
      "id": "wX5jUvUsiC5z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random embedding initialization for OOV words\n",
        "\n",
        "Then we will learn them by training"
      ],
      "metadata": {
        "id": "aVZmR_dHkgaX"
      },
      "id": "aVZmR_dHkgaX"
    },
    {
      "cell_type": "code",
      "source": [
        "for i, word in enumerate(vocab):\n",
        "    if word in twitter_glove:\n",
        "        embedding_matrix[i] = twitter_glove[word]\n",
        "    else:\n",
        "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))"
      ],
      "metadata": {
        "id": "wrGEtIJjiI0C"
      },
      "id": "wrGEtIJjiI0C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Create Keras Embedding layer"
      ],
      "metadata": {
        "id": "86ln5EO9jrZN"
      },
      "id": "86ln5EO9jrZN"
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    input_dim=len(vocab),\n",
        "    output_dim=embedding_dim,\n",
        "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=True  # Allow the model to adapt embeddings during training\n",
        "                    # So, OOV vectors will be learned to something more meaningful which are currently initialized radnomly\n",
        "                    # This also adapts pre-trained embeddings for our specific task (Should be a problme??)\n",
        ")"
      ],
      "metadata": {
        "id": "X6CO1wr1jsxl"
      },
      "execution_count": null,
      "outputs": [],
      "id": "X6CO1wr1jsxl"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fef9ca24",
        "2a4faa3e",
        "a66a507a",
        "8058981d",
        "vEnqOYsCYlvU",
        "K7k51at6l6_D",
        "RxS1xQEAbOzF",
        "A16P9r0vggvr",
        "uRzg97PjhMNF",
        "_zMU_mosjYVz",
        "wX5jUvUsiC5z",
        "aVZmR_dHkgaX",
        "86ln5EO9jrZN"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}